library("plyr")                     # Load plyr package
library("dplyr")                    # Manipulate data
library("haven")                    # Load dta files
library("sf")                       # Spatial features
library("raster")                   # Raster data (but also shapefiles)
# Set directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/BIHS data")
# Import data file with administrative codes in Bangladesh
BIHS_admin <- read_dta("BIHSlocations.dta")
View(BIHS_admin)
# Set working directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/Administrative boundary shapefiles/bgd_adm_bbs_20201113_shp/bgd_adm_bbs_20201113_SHP")
# Load the shapefile
bgd_admin4 <- read_sf("bgd_admbnda_adm4_bbs_20201113.shp")
# Set working directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/ACLED")
# Import the ACLED data
acled_data <- read.csv("Bangladesh_ACLED.csv")
# Move longitude before latitude
acled_data <- acled_data %>% relocate(latitude, .after = longitude)
# Turn ACLED data frame into spatial points data frame
acled_data_sf <- st_as_sf(acled_data, coords = c(23:24))
# Set projection as the same for both objects - using the World Geodetic System 1984 CRS
st_crs(acled_data_sf) = 4326
st_crs(bgd_admin4) = 4326
# Have a look at the spatial objects
acled_data_sf
bgd_admin4
# Perform a spatial join between a spatial features data frame with ACLED data and Bangladesh shapefile. This function will check whether each spatial point (geographical coordinates) falls inside each polygon and return a data frame with successfully matched information from both spatial objects.
acled_geo <- st_join(acled_data_sf, bgd_admin4)
# Turn the spatial points data frame into a regular data frame
acled_geo <- as.data.frame(acled_geo)
# Export as csv file
write.csv(acled_geo, "ACLED_geo.csv")
View(acled_geo)
acled_geo1 <- acled_geo[ , !names(acled_geo) %in%
c("ADM4_REF","ADM4ALT1EN", "ADM4ALT2EN")]
acled_geo1[, c(33, 35, 37, 39)] <- substr(acled_geo1$ADM4_PCODE, 3, 10)
# Check that codes for admin 1 (division) follow the same conventions
unique(acled_geo1$ADM1_PCODE)
unique(BIHS_admin$dvcode)
head(acled_geo1$ADM1_PCODE)
head(BIHS_admin$dvcode)
View(acled_geo1)
# Set directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/BIHS data")
# Import data file with administrative codes in Bangladesh
BIHS_admin <- read_dta("BIHSlocations.dta")
# Set working directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/Administrative boundary shapefiles/bgd_adm_bbs_20201113_shp/bgd_adm_bbs_20201113_SHP")
# Load the shapefile
bgd_admin4 <- read_sf("bgd_admbnda_adm4_bbs_20201113.shp")
# Set working directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/ACLED")
# Import the ACLED data
acled_data <- read.csv("Bangladesh_ACLED.csv")
# Move longitude before latitude
acled_data <- acled_data %>% relocate(latitude, .after = longitude)
# Turn ACLED data frame into spatial points data frame
acled_data_sf <- st_as_sf(acled_data, coords = c(23:24))
st_crs(acled_data_sf) = 4326
st_crs(bgd_admin4) = 4326
# Have a look at the spatial objects
acled_data_sf
bgd_admin4
# Perform a spatial join between a spatial features data frame with ACLED data and Bangladesh shapefile. This function will check whether each spatial point (geographical coordinates) falls inside each polygon and return a data frame with successfully matched information from both spatial objects.
acled_geo <- st_join(acled_data_sf, bgd_admin4)
# Turn the spatial points data frame into a regular data frame
acled_geo <- as.data.frame(acled_geo)
acled_geo1 <- acled_geo[ , !names(acled_geo) %in%
c("ADM4_REF","ADM4ALT1EN", "ADM4ALT2EN")]
acled_geo1$ADM4_PCODE <- substr(acled_geo1$ADM4_PCODE, 3, 10)
acled_geo1$ADM3_PCODE <- substr(acled_geo1$ADM3_PCODE, 3, 10)
acled_geo1$ADM2_PCODE <- substr(acled_geo1$ADM2_PCODE, 3, 10)
acled_geo1$ADM1_PCODE <- substr(acled_geo1$ADM1_PCODE, 3, 10)
head(acled_geo1$ADM1_PCODE)
head(BIHS_admin$dvcode)
acled_geo1$ADM4_PCODE <- substr(acled_geo1$ADM4_PCODE, 3, 10)
acled_geo1$ADM4_PCODE <- as.numeric(acled_geo1$ADM4_PCODE)
# Back to character
acled_geo1$ADM4_PCODE <- as.character(acled_geo1$ADM4_PCODE)
names(BIHS_admin)[names(BIHS_admin) == "uncode"] <- "ADM4_PCODE"
acled_geo1$ADM4_PCODE <- as.character(acled_geo1$ADM4_PCODE)
BIHS_admin$ADM4_PCODE <- as.character(BIHS_admin$ADM4_PCODE)
BIHS_2 <- left_join(BIHS_admin, acled_geo1)
View(BIHS_2)
View(BIHS_admin)
View(acled_geo1)
acled_p4 <- unique(acled_geo1$ADM4_PCODE)
BIHS_p4 <- unique(BIHS_admin$ADM4_PCODE)
View(BIHS_2)
View(BIHS_2)
View(BIHS_admin)
BIHS_admin2_p4 <- unique(BIHS_2)
BIHS_admin2_p4 <- unique(BIHS_2$ADM4_PCODE)
View(BIHS_2)
library("plyr")                     # Load plyr package
library("dplyr")                    # Manipulate data
library("haven")                    # Load dta files
library("sf")                       # Spatial features
library("raster")                   # Raster data (but also shapefiles)
# Set directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/BIHS data")
# Import data file with administrative codes in Bangladesh
BIHS_admin <- read_dta("BIHSlocations.dta")
# Set working directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/Administrative boundary shapefiles/bgd_adm_bbs_20201113_shp/bgd_adm_bbs_20201113_SHP")
# Load the shapefile
bgd_admin4 <- read_sf("bgd_admbnda_adm4_bbs_20201113.shp")
# Set working directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/ACLED")
# Import the ACLED data
acled_data <- read.csv("Bangladesh_ACLED.csv")
# Move longitude before latitude
acled_data <- acled_data %>% relocate(latitude, .after = longitude)
# Turn ACLED data frame into spatial points data frame
acled_data_sf <- st_as_sf(acled_data, coords = c(23:24))
# Set projection as the same for both objects - using the World Geodetic System 1984 CRS
st_crs(acled_data_sf) = 4326
st_crs(bgd_admin4) = 4326
# Have a look at the spatial objects
acled_data_sf
bgd_admin4
# Perform a spatial join between a spatial features data frame with ACLED data and Bangladesh shapefile. This function will check whether each spatial point (geographical coordinates) falls inside each polygon and return a data frame with successfully matched information from both spatial objects.
acled_geo <- st_join(acled_data_sf, bgd_admin4)
# Turn the spatial points data frame into a regular data frame
acled_geo <- as.data.frame(acled_geo)
# Export as csv file - this csv file matches conflict events from ACLED with administrative level 4 codes from the shapefile from OCHA
write.csv(acled_geo, "ACLED_geo.csv")
# Create a new data frame with compatible codes
# Remove unnecessary columns
acled_geo1 <- acled_geo[ , !names(acled_geo) %in%
c("ADM4_REF","ADM4ALT1EN", "ADM4ALT2EN")]
# Rename the code identifiers for administrative levels - remove BD
acled_geo1$ADM4_PCODE <- substr(acled_geo1$ADM4_PCODE, 3, 10)
acled_geo1$ADM3_PCODE <- substr(acled_geo1$ADM3_PCODE, 3, 10)
acled_geo1$ADM2_PCODE <- substr(acled_geo1$ADM2_PCODE, 3, 10)
acled_geo1$ADM1_PCODE <- substr(acled_geo1$ADM1_PCODE, 3, 10)
# Check that codes for admin 1 (division) follow the same conventions
head(acled_geo1$ADM1_PCODE)
head(BIHS_admin$dvcode)
# Yes - they follow two-digit convention
# Remove the first 2 digits from the union code (those refer to admin level 1)
acled_geo1$ADM4_PCODE <- substr(acled_geo1$ADM4_PCODE, 3, 10)
# This is correct for most cases. However, whenever "0" is the first digit, we should delete it
# Change to numeric - this will drop 0 at the front
acled_geo1$ADM4_PCODE <- as.numeric(acled_geo1$ADM4_PCODE)
# Back to character
acled_geo1$ADM4_PCODE <- as.character(acled_geo1$ADM4_PCODE)
# Rename columns in BIHS_admin before the join
names(BIHS_admin)[names(BIHS_admin) == "uncode"] <- "ADM4_PCODE"
# Back to character for compatibility of the merge
acled_geo1$ADM4_PCODE <- as.character(acled_geo1$ADM4_PCODE)
BIHS_admin$ADM4_PCODE <- as.character(BIHS_admin$ADM4_PCODE)
# Left join - this data will assing all conflict events to the unions in which BIHS households are located
BIHS_2 <- left_join(BIHS_admin, acled_geo1)
# Note that some unions did not have a any conflict events, while others had multiple conflict events.
# Have a look at unique values of admin 4 in both data frames
acled_p4 <- unique(acled_geo1$ADM4_PCODE)
BIHS_p4 <- unique(BIHS_admin$ADM4_PCODE)
# ACLED data covers 1358 unique unions while BIHS covers 275 unique unions
# Remove ugly columns
acled_geo1 <- acled_geo1[ , !names(acled_geo1) %in%
c("date","validTo", "validOn")]
# Export ACLED data as a csv file where the "ADM4_PCODE" column corresponds to the "uncode" column from the BIHS data.
write.csv(acled_geo1, "ACLED_geo1.csv")
rm(list = ls())
library("ncdf4")                    # package for netcdf manipulation
library("raster")                   # package for raster manipulation
library("SPEI")                     # SPEI construction
# Set the working directory to netCDF files
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/CRU Version 4 Climate Data/PET (Potential evapo-transpiration)/Raw data")
# Set the working directory to netCDF files
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Multiple Crises Poverty Nigeria/Data/CRU Version 4 Climate Data/PET (Potential evapo-transpiration)/Raw data")
# Open the netCDF file. This has already been merged and covers the period 1981-2020. It contains 480 time steps (12 months across 40 years)
pet_data <- nc_open("merged_pet.nc", write=FALSE, readunlim=TRUE, verbose=FALSE,
auto_GMT=TRUE, suppress_dimvals=FALSE, return_on_error=FALSE )
# Data has three dimensions: lon, lat, and time. The variable of interest is "pet"
# Extract variables - three dimensions
lon <- ncvar_get(pet_data, "lon")                # longitude
lat <- ncvar_get(pet_data, "lat", verbose = F)   # latitude
t <- ncvar_get(pet_data, "time")                 # time
# Inspect the units that are used in the time dimension
tunits <- ncatt_get(pet_data,"time","units")
tunits
# Store the dimensions of the time variable
nt <- dim(t)
nt
# 480 time units
# Look at the first few entries from the longitude variable
head(lat) # 360 values (1st:-89.75) with 0.5 unit spacing
head(lon) # 720 values(1st: -179.75) with 0.5 unit spacing
head(t)
# Extract the variable of interest (potential evapo-transpiration)
pet.array <- ncvar_get(pet_data, "pet") # store the data in a 3-dimensional array
# Checking the dimensions of the array
dim(pet.array)
# [1] 720 360 480
# 720 longitudes, 360 latitudes, and 480 time units (12 months across 40 years)
# See what fill value was used for missing data
fillvalue <- ncatt_get(pet_data, "pet", "_FillValue")
fillvalue
#[1] 9.96921e+36
# Replace missing values with the usual "NA"
pet.array[pet.array == fillvalue$value] <- NA
# Note: the array looks like it has loads of missing values. Check?
head(pet.array)
# Clear all
#rm(list = ls())
# Close the netCDF file
nc_close(pet_data)
# Convert time -- split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.numeric(unlist(tdstr)[2])
tday <- as.numeric(unlist(tdstr)[3])
tyear <- as.numeric(unlist(tdstr)[1])
#chron(time,origin=c(tmonth, tday, tyear))# note: this function does not seem to work with non-numeric or non-character values
# Check the number of non-NA values
length(na.omit(as.vector(pet.array[,,1])))
# [1] 66501
# Create a matrix of lon-lat pairs
lonlat <- as.matrix(expand.grid(lon,lat))
dim(lonlat)
# Make a vector with values for PET
pet_vec <- as.vector(pet.array)
length(pet_vec)
# reshape the vector into a matrix
pet_mat <- matrix(pet_vec, nrow=720*360, ncol=nt)
dim(pet_mat)
# Inspect the head of the matrix (excluding missing values)
head(na.omit(pet_mat))
# Create a dataframe using the lon-lat matrix
pet_df <- data.frame(cbind(lonlat,pet_mat))
# Assign names according to the original time dimension of the data (days since 1900-1-1)
names(pet_df) <- c("lon","lat", t)
# options(width=96)
head(na.omit(pet_df, 20))
# Now we have a data frame where each column is a point in time (12 months over 40 years) - need to break those down into month and year
# Create a matrix of month-year combinations
months <- 1:12
years <- 1981:2020
month_names <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
month_year <- as.matrix(expand.grid(months,years))
dim(month_year)
# Make a list with all combinations of months and years
timeref <- list()
# Need to start the loop with the year then month
for (year in years) {
timeref[[length(timeref)+1]] <- paste(year, month_names)
}
# Turn a list into a vector
timeref_vector <- unlist(timeref)
# Assign month-year combinations as column names for identification
names(pet_df) <- c("lon", "lat", timeref_vector)
# Set working directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/Administrative boundary shapefiles/bgd_adm_bbs_20201113_shp/bgd_adm_bbs_20201113_SHP")
# Load the shapefile
bgd_admin4 <- read_sf("bgd_admbnda_adm4_bbs_20201113.shp")
extent(bgd_admin4)
# Manually adjust extent
pet_df = pet_df[pet_df$lon >= 87.75 & pet_df$lon <= 92.75,]
pet_df = pet_df[pet_df$lat >= 20.25 & pet_df$lat <= 26.75,]
# Make a new data frame in case something goes wrong
pet_geo <- pet_df
# Make row names nicer
rownames(pet_geo) <- 1:nrow(pet_geo)
# Remove massive data files to free up space
rm(pet_df, pet_data, pet_vec, pet.array)
# Change daily values to monthly - multiply by 30.
pet_geo[,3:482] <- pet_geo[,3:482]*30
# Set the working directory to .dat files as an experiment
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Multiple Crises Poverty Nigeria/Data/CRU Version 4 Climate Data/PET (Potential evapo-transpiration)/Raw data")
# Open the netCDF file. This has already been merged and covers the period 1981-2020. It contains 480 time steps (12 months across 40 years)
pre_data <- nc_open("merged_pre.nc", write=FALSE, readunlim=TRUE, verbose=FALSE,
auto_GMT=TRUE, suppress_dimvals=FALSE, return_on_error=FALSE )
# Set the working directory to .dat files as an experiment
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Multiple Crises Poverty Nigeria/Data/CRU Version 4 Climate Data/PRE (Precipitation)/Raw data")
# Open the netCDF file. This has already been merged and covers the period 1981-2020. It contains 480 time steps (12 months across 40 years)
pre_data <- nc_open("merged_pre.nc", write=FALSE, readunlim=TRUE, verbose=FALSE,
auto_GMT=TRUE, suppress_dimvals=FALSE, return_on_error=FALSE )
# Data has three dimensions: lon, lat, and time. The variable of interest is "pet"
# Extract variables - three dimensions
lon <- ncvar_get(pre_data, "lon")                # longitude
lat <- ncvar_get(pre_data, "lat", verbose = F)   # latitude
t <- ncvar_get(pre_data, "time")                 # time
# Inspect the units that are used in the time dimension
tunits <- ncatt_get(pre_data,"time","units")
tunits
# Store the dimensions of the time variable
nt <- dim(t)
nt
# 480 time units
# Look at the first few entries from the longitude variable
head(lat) # 360 values (1st:-89.75) with 0.5 unit spacing
head(lon) # 720 values(1st: -179.75) with 0.5 unit spacing
head(t)
# Extract the variable of interest (potential evapo-transpiration)
pre.array <- ncvar_get(pre_data, "pre") # store the data in a 3-dimensional array
# Checking the dimensions of the array
dim(pre.array)
# [1] 720 360 480
# 720 longitudes, 360 latitudes, and 480 time units (12 months across 40 years)
# See what fill value was used for missing data
fillvalue <- ncatt_get(pre_data, "pre", "_FillValue")
fillvalue
#[1] 9.96921e+36
# Make some space by removing the massive pet_data
rm(pet_data)
# Need more space - keep just objects for Nigeria
rm(pet_mat, pet_df)
rm(pet.array, pet_vec)
# Replace missing values with the usual "NA"
pre.array[pre.array == fillvalue$value] <- NA
# Note: the array looks like it has loads of missing values. Check?
head(pre.array)
# Clear all
#rm(list = ls())
# Close the netCDF file
nc_close(pre_data)
# Make a vector with values for PRE
pre_vec <- as.vector(pre.array)
length(pre_vec)
# reshape the vector into a matrix
pre_mat <- matrix(pre_vec, nrow=720*360, ncol=nt)
dim(pre_mat)
# Create a dataframe using the lon-lat matrix
pre_df <- data.frame(cbind(lonlat,pre_mat))
# Assign names according to the original time dimension of the data (days since 1900-1-1)
names(pre_df) <- c("lon","lat", t)
# options(width=96)
head(na.omit(pre_df, 20))
# Now we have a data frame where each column is a point in time (12 months over 40 years) - need to break those down into month and year
# Assign month-year combinations as column names for identification
names(pre_df) <- c("lon", "lat", timeref_vector)
# Make a data frmae with just 0.5 degree coordinates that fall within Bangladesh
BG_coords <- pet_geo[,c("lon", "lat")]
# Left join with the precipitation data frame
pre_geo <- left_join(BG_coords, pre_df)
# Remove large unnecessary objects to free up space
rm(pre_data, pre_df, pre_mat, pre_vec, pre.array)
# Create a data frame for PET within Bangladesh that has only columns relating to PET (and not info from the shapefile)
pet_bgd <- pet_geo[,1:482]
# Subtract PET from precipitation
water_balance <- pre_geo[,3:482] - pet_bgd[,3:482]
# Append the longitude and latitude
water_balance['lon'] <- pre_geo$lon
water_balance['lat'] <- pre_geo$lat
# Move longitude and latitude to the front
water_balance <- water_balance %>%
relocate(lat)
water_balance <- water_balance %>%
relocate(lon)
# First we need to turn the water balance data frame into a time series
# Turn lon lat into characters and add column where both are displayed
BG_coords$lon <- as.character(BG_coords$lon)
BG_coords$lat <- as.character(BG_coords$lat)
BG_coords$lonlat <- paste(BG_coords$lon, BG_coords$lat, sep = " ")
# Add the column for lon-lat
water_balance$lonlat <- BG_coords$lonlat
# Delet individual lon-lat measurements
water_balance <- water_balance[,3:483]
# Move lon-lat to the front
water_balance <- water_balance %>%
relocate(lonlat)
# Transpose to make time-series a column
water_balance_long <- as.data.frame(t(water_balance))
# Make column names lon-lat
names(water_balance_long) <- water_balance_long[1,]
# Make columns numeric values
water_balance_long <- sapply(water_balance_long, as.numeric)
# Remove lon-lat
water_balance_long <- water_balance_long[-1,]
# Declare water balance a time series, specifying the start and end point
water_balance_long <- ts(water_balance_long, start=c(1981,1), end=c(2020,12), frequency=12)
# Store the sequence of years in a vector
s_years <- 2010:2020
# Create a sequence of months
all_months <- vec_rep(month_names, 40)
library("vctrs")                    # Vector operations
# Store the sequence of years in a vector
s_years <- 2010:2020
# Create a sequence of months
all_months <- vec_rep(month_names, 40)
# Create a sequence of years
all_years <- rep(years,each=12)
# Create a list to store fitted values
fitted_list <- list()
# Loop time
for (i in s_years) {
spei_list <- spei(water_balance_long, 12, kernel = list(type = 'rectangular', shift = 0),  # Calculate 12-month SPEI
distribution = 'log-Logistic', fit = 'ub-pwm', na.rm = TRUE,
ref.start=NULL, ref.end=c(i,12), x=FALSE, params=NULL)
fitted <- spei_list$fitted
fitted <- as.data.frame(fitted)
fitted$month <- all_months
fitted$year <- all_years
data <- fitted %>% filter(month =="Dec" & year==i)
data <- data[,1:154] # number of grid cells
data <- as.data.frame(t(data))
data$lon <- BG_coords$lon   # Assign lon and lat
data$lat <- BG_coords$lat
fitted_list[[i-2009]] <- data
}
# Prepare a data frame
spei_2010_2020 <- pet_geo[,1:2]
spei_2010_2020 <- fitted_list[[1]]
colnames(spei_2010_2020)[which(names(spei_2010_2020) == "V1")] <- "SPEI_2010"
values <- list()
for (i in s_years) {
values[[i-2009]] <- fitted_list[[i-2009]]$V1
}
spei_2010_2020$SPEI_2011 <- values[[2]]
spei_2010_2020$SPEI_2012 <- values[[3]]
spei_2010_2020$SPEI_2013 <- values[[4]]
spei_2010_2020$SPEI_2014 <- values[[5]]
spei_2010_2020$SPEI_2015 <- values[[6]]
spei_2010_2020$SPEI_2016 <- values[[7]]
spei_2010_2020$SPEI_2017 <- values[[8]]
spei_2010_2020$SPEI_2018 <- values[[9]]
spei_2010_2020$SPEI_2019 <- values[[10]]
spei_2010_2020$SPEI_2020 <- values[[11]]
# Rename rows
rownames(spei_2010_2020) <- 1:nrow(spei_2010_2020)
# Move longitude and latitude to the front
spei_2010_2020 <- spei_2010_2020 %>%
relocate(lat)
spei_2010_2020 <- spei_2010_2020 %>%
relocate(lon)
# Remove grid cells with NA values
spei_2010_2020 <- spei_2010_2020[!is.na(spei_2010_2020$SPEI_2010),]
# Make row names nices
rownames(spei_2010_2020) <- 1:nrow(spei_2010_2020)
# Export this as a csv file
write.csv(spei_2010_2020, "SPEI_2010_20_BGD.csv")
library("leaflet")                  # Interactive maps
library("htmlwidgets")              # HTML widgets
library("RColorBrewer")             # Nice colours
# Test for 2010
spei_2010 <- spei_2010_2020[,1:3]
r_drought <- rasterFromXYZ(spei_2010)
plot(r_drought)
pal1 <- colorNumeric(c("#B10026", "#E31A1C", "#FFFFCC", "#FC4E2A", "#FD8D3C", "#FEB24C", "#FED976", "#FFEDA0"), values(r_drought),
na.color = "transparent")
crs(r_drought) <- sp::CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
map5 <- leaflet() %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addRasterImage(r_drought, colors = pal1, opacity=0.7, project = FALSE)%>%
addLegend(pal = pal1, values = values(r_drought), title = "SPEI")
map5
map5 <- leaflet() %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addRasterImage(r_drought, colors = pal1, opacity=0.7, project = FALSE)%>%
addLegend(pal = pal1, values = values(r_drought), title = "SPEI")
map5
crs(r_drought) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
map5 <- leaflet() %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addRasterImage(r_drought, colors = pal1, opacity=0.7, project = FALSE)%>%
addLegend(pal = pal1, values = values(r_drought), title = "SPEI")
map5
