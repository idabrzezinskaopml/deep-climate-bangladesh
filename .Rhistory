ADM4_PCODE == "386115" ~ "386109",      # 9
ADM4_PCODE == "696320" ~ "696312",      # 10
ADM4_PCODE == "496111" ~ "496106",      # 11
ADM4_PCODE == "855826" ~ "855816",      # 12
ADM4_PCODE == "857317" ~ "857309",      # 13
ADM4_PCODE == "857656" ~ "857650"))     # 14
# Transfer all other non-missing codes
bgd_admin4$uncode[is.na(bgd_admin4$uncode)] <- bgd_admin4$ADM4_PCODE
View(bgd_admin4)
bgd_admin4<- bgd_admin4[ , !names(bgd_admin4) %in%
c("ADM4_PCODE")]
names(bgd_spei_newcodes)[names(bgd_spei_newcodes) == 'uncode'] <- "ADM4_PCODE"
names(bgd_admin4)[names(bgd_admin4) == 'uncode'] <- "ADM4_PCODE"
# Move admin 4 code to the front
bgd_admin4 <- bgd_admin4 %>%
relocate("ADM4_PCODE")
# Calculate the mean value of SPEI in each union (administrative level 4) - keep identifiers for admin level 4
bgd_spei <- exact_extract(raster_spei, bgd_admin4, fun = c('mean'), append_cols=c("ADM4_PCODE", "ADM4_EN"))
# Check that there are no missing values -3 NA values (coastal areas which the grid did not cover )
summary(bgd_spei)
BIHS_drought <- left_join(BIHS_admin, bgd_spei_newcodes)
# Remove "mean" from column names
names(BIHS_drought) <- sub("^mean.", "", names(BIHS_drought))
summary(BIHS_drought)
# Set working directory
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/")
# Import SPEI data for 2010-2020 covering all of Bangladesh on a regular 0.5 degrees grid
spei_geo <- read.csv("Code/SPEI_2010_20_BGD.csv")
# Load shapefile with administrative level 4 for Bangladesh
bgd_admin4 <- read_sf("Data/Administrative boundary shapefiles/bgd_adm_bbs_20201113_shp/bgd_adm_bbs_20201113_SHP/bgd_admbnda_adm4_bbs_20201113.shp")
# Load BIHS location data
BIHS_admin <- read_dta("Data/BIHS data/BIHSlocations.dta")
# Remove unnecessary columns
spei_geo <- spei_geo[,2:14]
# Turn data frame with SPEI values into a raster - this creates a "brick" where values of SPEI for all years are on top of one another
raster_spei <- rasterFromXYZ(spei_geo)
# Make sure the CRS is consistent with shapefile
crs(raster_spei) <- "+proj=longlat +datum=WGS84 +no_defs"
# Rename the code identifiers for administrative levels - remove BD
bgd_admin4$ADM4_PCODE <- substr(bgd_admin4$ADM4_PCODE, 3, 10)
# Remove the first 2 digits from the union code (those refer to admin level 1)
bgd_admin4$ADM4_PCODE <- substr(bgd_admin4$ADM4_PCODE, 3, 10)
# This is correct for most cases. However, whenever "0" is the first digit, we should delete it
# Change to numeric - this will drop 0 at the front
bgd_admin4$ADM4_PCODE <- as.numeric(bgd_admin4$ADM4_PCODE)
# Back to character
bgd_admin4$ADM4_PCODE <- as.character(bgd_admin4$ADM4_PCODE)
# Rename columns in BIHS_admin before the join
names(BIHS_admin)[names(BIHS_admin) == "uncode"] <- "ADM4_PCODE"
# Back to character for compatibility of the merge
BIHS_admin$ADM4_PCODE <- as.character(BIHS_admin$ADM4_PCODE)
# Create another data frame with values from the Bangladesh shapefile to replace with new matching codes. Here I replace shapefile codes with codes from BIHS.
bgd_admin4 <- bgd_admin4 %>%
mutate(uncode = case_when(
ADM4_PCODE == "199417" ~ "193617",      # 1
ADM4_PCODE == "198110" ~ "198104",      # 2
ADM4_PCODE == "338621" ~ "338609",      # 3
ADM4_PCODE == "354311" ~ "354306",      # 4
ADM4_PCODE == "824795" ~ "827395",      # 5
ADM4_PCODE == "932580" ~ "935780",      # 6
ADM4_PCODE == "108556" ~ "102056",      # 7
ADM4_PCODE == "105413" ~ "105409",      # 8
ADM4_PCODE == "386115" ~ "386109",      # 9
ADM4_PCODE == "696320" ~ "696312",      # 10
ADM4_PCODE == "496111" ~ "496106",      # 11
ADM4_PCODE == "855826" ~ "855816",      # 12
ADM4_PCODE == "857317" ~ "857309",      # 13
ADM4_PCODE == "857656" ~ "857650"))     # 14
# Transfer all other non-missing codes
bgd_admin4$uncode[is.na(bgd_admin4$uncode)] <- bgd_admin4$ADM4_PCODE
# Drop the original column and rename the new one
bgd_admin4<- bgd_admin4[ , !names(bgd_admin4) %in%
c("ADM4_PCODE")]
names(bgd_admin4)[names(bgd_admin4) == 'uncode'] <- "ADM4_PCODE"
# Move admin 4 code to the front
bgd_admin4 <- bgd_admin4 %>%
relocate("ADM4_PCODE")
# Calculate the mean value of SPEI in each union (administrative level 4) - keep identifiers for admin level 4
bgd_spei <- exact_extract(raster_spei, bgd_admin4, fun = c('mean'), append_cols=c("ADM4_PCODE", "ADM4_EN"))
# Check that there are no missing values -3 NA values (coastal areas which the grid did not cover )
summary(bgd_spei)
# Left join - all codes should match
BIHS_drought <- left_join(BIHS_admin, bgd_spei_newcodes)
# Remove "mean" from column names
names(BIHS_drought) <- sub("^mean.", "", names(BIHS_drought))
summary(BIHS_drought)
View(BIHS_drought)
st_write(bgd_admin4, "bgd_admin4_bihs_harmonised.shp")
st_write(bgd_admin4, "C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/Harmonised shapefile admin 4bgd_admin4_bihs_harmonised.shp")
rm(list=ls())
library("plyr")                     # Load plyr package
library("dplyr")                   # Load dplyr package
library("readr")                    # Load readr package
library("tidyr")                    # Data manipulation
library("stargazer")                # For LaTeX tables
library("AER")                      # Robust standard errors
library("reshape2")                 # reshape long
library("haven")                    # Load dta files
library("foreign")                  # Export dta files
library("vctrs")                    # Vector operations
library("geosphere")                # To find distance between geographic coordinates
library("plm")                      # For panel data and first difference
library("sf")                       #Spatial vector data
# Set working directory for this chunk
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/BIHS data")
# Import .dta file with BIHS + drought and flood variables - to match with what Vidya has been using for regression.
BPanel_reg <- read_dta('BPanel_short.dta')
View(BPanel_reg)
# Import shapefile for administrative level 4 harmonised with BIHS
bgd_admin4 <- read_sf("C:\Users\idabr\OneDrive - Oxford Policy Management Limited\DEEP Conflict Climate Bangladesh\Data\Harmonised shapefile admin 4\bgd_admin4_bihs_harmonised.shp")
# Import shapefile for administrative level 4 harmonised with BIHS
bgd_admin4 <- read_sf("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/Harmonised shapefile admin 4/bgd_admin4_bihs_harmonised.shp")
# Find the cenroid of the union
centroid_df <- st_centroid(bgd_admin4)
View(bgd_admin4)
# Leave only identifier for admin 4
centroid_df <- centroid_df[, c("ADM4_PCODE", "ADM4_EN", "geometry")]
# Create buffer zones of 50km and 100km around the centroid and calculate the mean
points_buffer_50km <- st_buffer(centroid_df, dist = 50000)
points_buffer_100km <- st_buffer(centroid_df, dist = 100000)
# Make a list of data frames
buffer_list <- list(points_buffer_100km, points_buffer_50km)
View(points_buffer_100km)
View(BPanel_reg)
View(BPanel_reg)
BPanel_reg <- BPanel_reg %>%
mutate(dum_drought = case_when(
SPEI_2010 < -1.5 & year ==2011 ~ 1,
SPEI_2015 < -1.5 & year ==2015 ~ 1,
SPEI_2019 < -1.5 & year ==2019 ~ 0,
else ~ 0
BPanel_reg <- BPanel_reg %>%
mutate(dum_drought = case_when(
SPEI_2010 < -1.5 & year ==2011 ~ 1,
SPEI_2015 < -1.5 & year ==2015 ~ 1,
SPEI_2019 < -1.5 & year ==2019 ~ 1,
))
View(BPanel_reg)
View(BPanel_reg)
summary(BPanel_reg$dum_drought)
BPanel_reg <- BPanel_reg %>%
mutate(dum_drought = case_when(
(SPEI_2010 < -1.5 & year ==2011) ~ 1,
(SPEI_2015 < -1.5 & year ==2015) ~ 1,
(SPEI_2019 < -1.5 & year ==2019) ~ 1,
))
summary(BPanel_reg$dum_drought)
BPanel_reg <- BPanel_reg %>%
mutate(dum_drought = case_when(
(SPEI_2010 < -1.5 & year ==2011) ~ 1,
(SPEI_2015 < -1.5 & year ==2015) ~ 1,
(SPEI_2019 < -1.5 & year ==2019) ~ 1,
.default = 0
))
summary(BPanel_reg$dum_drought)
BPanel_reg <- BPanel_reg %>%
mutate(dum_drought = case_when(
(SPEI_2011 < -1.5 & year ==2011) | (SPEI_2015 < -1.5 & year ==2015) | (SPEI_2019 < -1.5 & year ==2019) ~ 1,
.default = 0
))
summary(BPanel_reg$dum_drought)
BPanel_reg$dum_drought[10,]
BPanel_reg[10,]
BPanel_reg$flood_2011 <- rowSums(BPanel_reg[, c("f_20110721", "f_20110815", "f_20110815_2", "f_20110905")])
summary(BPanel_reg$flood_2011)
cols_to_sum_2011 <- grep("^_2011", names(BPanel_reg))
cols_to_sum_2011 <- grep("^f_2011", names(BPanel_reg))
BPanel_reg$flood_2011 <- rowSums(BPanel_reg[, cols_to_sum])
BPanel_reg$flood_2011 <- rowSums(BPanel_reg[, cols_to_sum_2011])
summary(BPanel_reg$flood_2011)
cols_to_sum_2011 <- grep("^f_2011", names(BPanel_reg))
# Sum the selected columns row-wise
BPanel_reg$flood_2011 <- rowSums(BPanel_reg[, cols_to_sum_2011])
# 2015
cols_to_sum_2015 <- grep("^f_2015", names(BPanel_reg))
# Sum the selected columns row-wise
BPanel_reg$flood_2015 <- rowSums(BPanel_reg[, cols_to_sum_2015])
# 2018
cols_to_sum_2018 <- grep("^f_2018", names(BPanel_reg))
# Sum the selected columns row-wise
BPanel_reg$flood_2018 <- rowSums(BPanel_reg[, cols_to_sum_2018])
BPanel_reg <- BPanel_reg %>%
mutate(dum_flood = case_when(
(flood_2011 > 0 & year ==2011) | (flood_2015 > 0 & year ==2015) | (flood_2018 >0 & year ==2019) ~ 1,
.default = 0
))
summary(BPanel_reg$dum_flood)
# Region
BPanel_reg$trend_region <- BPanel_reg$region*BPanel_reg$year
# Create a dummy for whether there was a presence of conflict
BPanel_reg$dum_violence <- 0
# Replace values if there were any fatalities
BPanel_reg$dum_violence[BPanel_reg$fatalitiesnew > 0] <- 1
BPanel_reg_50km <- left_join(BPanel_reg, points_buffer_50km)
names(bgd_admin4)[names(bgd_admin4) == "ADM4_PCODE"] <- "uncode"
rm(list=ls())
# Set working directory for this chunk
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/BIHS data")
# Import .dta file with BIHS + drought and flood variables - to match with what Vidya has been using for regression.
BPanel_reg <- read_dta('BPanel_short.dta')
# Import shapefile for administrative level 4 harmonised with BIHS
bgd_admin4 <- read_sf("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/Harmonised shapefile admin 4/bgd_admin4_bihs_harmonised.shp")
# Rename "ADM4_PCODE" as "uncode"
names(bgd_admin4)[names(bgd_admin4) == "ADM4_PCODE"] <- "uncode"
# Find the cenroid of the union
centroid_df <- st_centroid(bgd_admin4)
# Import shapefile for administrative level 4 harmonised with BIHS
bgd_admin4 <- read_sf("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/Harmonised shapefile admin 4/bgd_admin4_bihs_harmonised.shp")
# Rename "ADM4_PCODE" as "uncode"
names(bgd_admin4)[names(bgd_admin4) == "ADM4_PCODE"] <- "uncode"
# Find the cenroid of the union
centroid_df <- st_centroid(bgd_admin4)
# Leave only identifier for admin 4
centroid_df <- centroid_df[, c("uncode", "ADM4_EN", "geometry")]
# Create buffer zones of 50km and 100km around the centroid and calculate the mean
points_buffer_50km <- st_buffer(centroid_df, dist = 50000)
points_buffer_100km <- st_buffer(centroid_df, dist = 100000)
# Make a list of data frames
#buffer_list <- list(points_buffer_100km, points_buffer_50km)
# Set dummy equal to 1 if household experiences a drought in the survey year
BPanel_reg <- BPanel_reg %>%
mutate(dum_drought = case_when(
(SPEI_2011 < -1.5 & year ==2011) | (SPEI_2015 < -1.5 & year ==2015) | (SPEI_2019 < -1.5 & year ==2019) ~ 1,
.default = 0
))
# Have a look
summary(BPanel_reg$dum_drought)
# Replace for each survey year: wave 1 (2010), wave 2 (2012), wave 3 (2015), wave 4 (2019) - flood (union-level)
# Get column names for flood dates
colnames(BPanel_reg)
# First a variable capturing flood conditions if there was more than one in a year
cols_to_sum_2011 <- grep("^f_2011", names(BPanel_reg))
# Sum the selected columns row-wise
BPanel_reg$flood_2011 <- rowSums(BPanel_reg[, cols_to_sum_2011])
# 2015
cols_to_sum_2015 <- grep("^f_2015", names(BPanel_reg))
# Sum the selected columns row-wise
BPanel_reg$flood_2015 <- rowSums(BPanel_reg[, cols_to_sum_2015])
# 2018
cols_to_sum_2018 <- grep("^f_2018", names(BPanel_reg))
# Sum the selected columns row-wise
BPanel_reg$flood_2018 <- rowSums(BPanel_reg[, cols_to_sum_2018])
# Create a dummy based on flood presence in a particular year
BPanel_reg <- BPanel_reg %>%
mutate(dum_flood = case_when(
(flood_2011 > 0 & year ==2011) | (flood_2015 > 0 & year ==2015) | (flood_2018 >0 & year ==2019) ~ 1,
.default = 0
))
# Have a look
summary(BPanel_reg$dum_flood)
# Region
BPanel_reg$trend_region <- BPanel_reg$region*BPanel_reg$year
# Create a dummy for whether there was a presence of conflict
BPanel_reg$dum_violence <- 0
# Replace values if there were any fatalities
BPanel_reg$dum_violence[BPanel_reg$fatalitiesnew > 0] <- 1
# Rename "ADM4_PCODE" as "uncode"
names(bgd_admin4)[names(bgd_admin4) == "ADM4_PCODE"] <- "uncode"
# Match compatible shapefile with BIHS data
BPanel_reg_50km <- left_join(BPanel_reg, points_buffer_50km)
# Change uncode to character
BPanel_reg$uncode <- as.character(BPanel_reg$uncode)
# Match compatible shapefile with BIHS data
BPanel_reg_50km <- left_join(BPanel_reg, points_buffer_50km)
View(BPanel_reg_50km)
View(BPanel_reg_50km)
BPanel_reg_50km
# Match compatible shapefile with BIHS data
BPanel_reg_50km <- left_join(BPanel_reg, points_buffer_50km)
summary(BPanel_reg_50km)
BPanel_reg_100km <- left_join(BPanel_reg, points_buffer_100km)
crs(bgd_admin4) <- 32646
print(st_crs(bgd_admin4))
st_crs(bgd_admin4) <- 32646
st_transform(bgd_admin4) <- 32646
bgd_admin4 <- st_set_crs(bgd_admin4, 32646)
print(st_crs(bgd_admin4))
# Set working directory for this chunk
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/BIHS data")
# Import .dta file with BIHS + drought and flood variables - to match with what Vidya has been using for regression.
BPanel_reg <- read_dta('BPanel_short.dta')
# Import shapefile for administrative level 4 harmonised with BIHS
bgd_admin4 <- read_sf("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/Harmonised shapefile admin 4/bgd_admin4_bihs_harmonised.shp")
# Rename "ADM4_PCODE" as "uncode"
names(bgd_admin4)[names(bgd_admin4) == "ADM4_PCODE"] <- "uncode"
# Find the cenroid of the union
centroid_df <- st_centroid(bgd_admin4)
# Leave only identifier for admin 4
centroid_df <- centroid_df[, c("uncode", "ADM4_EN", "geometry")]
# Set geographical projection that uses meters
print(st_crs(bgd_admin4))
bgd_admin4 <- st_set_crs(bgd_admin4, 32646)
# Create buffer zones of 50km and 100km around the centroid and calculate the mean
points_buffer_50km <- st_buffer(centroid_df, dist = 50000)
points_buffer_100km <- st_buffer(centroid_df, dist = 100000)
print(st_crs(bgd_admin4))
# Set dummy equal to 1 if household experiences a drought in the survey year
BPanel_reg <- BPanel_reg %>%
mutate(dum_drought = case_when(
(SPEI_2011 < -1.5 & year ==2011) | (SPEI_2015 < -1.5 & year ==2015) | (SPEI_2019 < -1.5 & year ==2019) ~ 1,
.default = 0
))
# Have a look
summary(BPanel_reg$dum_drought)
# Replace for each survey year: wave 1 (2010), wave 2 (2012), wave 3 (2015), wave 4 (2019) - flood (union-level)
# Get column names for flood dates
colnames(BPanel_reg)
# First a variable capturing flood conditions if there was more than one in a year
cols_to_sum_2011 <- grep("^f_2011", names(BPanel_reg))
# Sum the selected columns row-wise
BPanel_reg$flood_2011 <- rowSums(BPanel_reg[, cols_to_sum_2011])
# 2015
cols_to_sum_2015 <- grep("^f_2015", names(BPanel_reg))
# Sum the selected columns row-wise
BPanel_reg$flood_2015 <- rowSums(BPanel_reg[, cols_to_sum_2015])
# 2018
cols_to_sum_2018 <- grep("^f_2018", names(BPanel_reg))
# Sum the selected columns row-wise
BPanel_reg$flood_2018 <- rowSums(BPanel_reg[, cols_to_sum_2018])
# Create a dummy based on flood presence in a particular year
BPanel_reg <- BPanel_reg %>%
mutate(dum_flood = case_when(
(flood_2011 > 0 & year ==2011) | (flood_2015 > 0 & year ==2015) | (flood_2018 >0 & year ==2019) ~ 1,
.default = 0
))
# Have a look
summary(BPanel_reg$dum_flood)
# Region
BPanel_reg$trend_region <- BPanel_reg$region*BPanel_reg$year
# Create a dummy for whether there was a presence of conflict
BPanel_reg$dum_violence <- 0
# Replace values if there were any fatalities
BPanel_reg$dum_violence[BPanel_reg$fatalitiesnew > 0] <- 1
BPanel_reg$uncode <- as.character(BPanel_reg$uncode)
BPanel_reg_centroids <- left_join(BPanel_reg, centroid_df)
points_within_50km <- st_join(BPanel_reg_centroids, points_buffer_50km, join = st_within, suffix = c("", "_buffer"))
View(BPanel_reg_centroids)
points_buffer_50km
BPanel_reg_centroids
BPanel_reg_centroids <- st_as_sf(BPanel_reg_centroids)
BPanel_reg_centroids
points_within_50km <- st_join(BPanel_reg_centroids, points_buffer_50km, join = st_within, suffix = c("", "_buffer"))
View(points_within_50km)
View(points_buffer_50km)
mean_values <- points_within %>%
group_by(geometry) %>%  # Group by buffer ID
summarize(across(c(c("poor", "fatalitiesany1119", "drought_any111915", "floods_any1119", "hhsize", "hhage", "hhage2", "hhfemale", "relig", "hhedupri", "hhemp_agri", "nfe", "logAV", "lsT", "remitany", "region", "dum_flood", "dum_drought", "dum_violence", "trend_region", "year")), mean, na.rm = TRUE))
mean_values <- points_within_50km %>%
group_by(geometry) %>%  # Group by buffer ID
summarize(across(c(c("poor", "fatalitiesany1119", "drought_any111915", "floods_any1119", "hhsize", "hhage", "hhage2", "hhfemale", "relig", "hhedupri", "hhemp_agri", "nfe", "logAV", "lsT", "remitany", "region", "dum_flood", "dum_drought", "dum_violence", "trend_region", "year")), mean, na.rm = TRUE))
mean_values <- points_within_50km %>%
group_by(geometry) %>%  # Group by buffer ID
summarize(across(c("poor", "fatalitiesany1119", "drought_any111915", "floods_any1119", "hhsize", "hhage", "hhage2", "hhfemale", "relig", "hhedupri", "hhemp_agri", "nfe", "logAV", "lsT", "remitany", "region", "dum_flood", "dum_drought", "dum_violence", "trend_region", "year")), mean, na.rm = TRUE)
View(points_within_50km)
View(BPanel_reg_centroids)
# Create a data frame for just the coordinate points
coordinates <- BPanel_reg[, c('longitude', 'latitude')]
View(BPanel_reg)
View(BPanel_reg_centroids)
centroid_fun <- function(x) {x %>% mutate(centroids = st_centroid(st_geometry(.)))}
# Function for extracting longitudes and latitudes
lonlat_fun <- function(x) {x %>% mutate(long=gsub(".*POINT (.+) ''.*", "\\1", centroids),
cords=str_split_fixed(long,', ',2),
longitude=as.numeric(gsub('c\\(','',cords[,1])),
latitude=as.numeric(gsub('\\)','',cords[,2])))}
# Apply functions to all data frames
BPanel_reg_centroids <- lapply(BPanel_reg_centroids, lonlat_fun)
View(BPanel_reg_centroids)
lonlat_fun <- function(x) {x %>% mutate(long=gsub(".*POINT (.+) ''.*", "\\1", geometry),
cords=str_split_fixed(long,', ',2),
longitude=as.numeric(gsub('c\\(','',cords[,1])),
latitude=as.numeric(gsub('\\)','',cords[,2])))}
# Apply functions to all data frames
BPanel_reg_centroids <- lapply(BPanel_reg_centroids, lonlat_fun)
BPanel_reg_centroids <- (BPanel_reg_centroids, lonlat_fun)
BPanel_reg_centroids <- lonlat_fun(BPanel_reg_centroids)
library("stringr")                  # String operators
BPanel_reg_centroids <- lonlat_fun(BPanel_reg_centroids)
View(BPanel_reg_centroids)
BPanel_reg <- lonlat_fun(BPanel_reg_centroids)
# Create a data frame for just the coordinate points
coordinates <- BPanel_reg[, c('longitude', 'latitude')]
# Put coordinates in a matrix
coordsmatrix <- data.matrix(coordinates)
View(coordinates)
# Create a data frame for just the coordinate points
coordinates <- BPanel_reg[, c('longitude', 'latitude')]
# Remove geometry
coordinates <- as.data.frame(coordinates[, c("longitude", "latitude")])
# Put coordinates in a matrix
coordsmatrix <- data.matrix(coordinates)
View(coordinates)
coordinates <- as.data.frame(coordinates[, c(-"geometry")])
coordinates <- as.data.frame(coordinates)
coordinates <- coordinates[,c("longitude", "latitude")]
# Put coordinates in a matrix
coordsmatrix <- data.matrix(coordinates)
# Create a distance matrix for points within 100km
res<- distm(coordsmatrix,fun=distGeo)<=100000
# Output is logical - either TRUE or FALSE depending on whether the condition is met
# Create a distance matrix for points within 50km
res_50km<- distm(coordsmatrix,fun=distGeo)<=50000
# Check survey years
unique(BPanel_reg$year)
# Prepare a data frame with just indicators of interest
bangladesh_vars <- BPanel_reg[, c("poor", "fatalitiesany1119", "drought_any111915", "floods_any1119", "hhsize", "hhage", "hhage2", "hhfemale", "relig", "hhedupri", "hhemp_agri", "nfe", "logAV", "lsT", "remitany", "region", "dum_flood", "dum_drought", "dum_violence", "trend_region", "year")]
# Get number of variables (number of columns) and observations
col_num <- as.numeric(ncol(bangladesh_vars))
col_num_l1 <- col_num-1   # number of columns less 1 for the loop
row_num <- as.numeric(nrow(BPanel_reg))
# Loop time
for (n in 1:col_num_l1) {
data_var <- bangladesh_vars[,c(n, col_num)]              # create a data frame with just the variable to be differenced + wave
list_vars <- list()                             # create a list to store differenced variables
var_name <- colnames(bangladesh_vars[,n])           # get column name to store
names(bangladesh_vars)[names(bangladesh_vars) == var_name] <- "temp_name"
for (i in 1: row_num) {                               # for each observation
list  <- which(res[i,],  arr.ind = T)           # make a list of observations that are within a 100km radius
newdata <- data_var[list,]                        # create a new data frame with just these observations
year_same <- data_var[i,]$year              # identify the survey wave associated with the central observation
newdata <- subset(newdata, newdata$year==year_same) # leave only a subset of observations that are within the same year
newdata[, 1] <- sapply(newdata[, 1], as.numeric) # convert column to numeric
names(newdata)[names(newdata) == var_name] <- "test"  # change column name before getting the mean
avgvar100km <- mean(newdata$test)           # find the average of the variable in the 100 km radius
list_vars[[i]] <- avgvar100km - bangladesh_vars[i,]$temp_name
}
BPanel_reg$var <- as.numeric(list_vars)
names(BPanel_reg)[names(BPanel_reg) == 'var'] <- paste(var_name, "100km", sep="_")
}
View(bangladesh_vars)
# Prepare a data frame with just indicators of interest
bangladesh_vars <- BPanel_reg[, c("poor", "fatalitiesany1119", "drought_any111915", "floods_any1119", "hhsize", "hhage", "hhage2", "hhfemale", "relig", "hhedupri", "hhemp_agri", "nfe", "logAV", "lsT", "remitany", "region", "dum_flood", "dum_drought", "dum_violence", "trend_region", "year")]
# Get number of variables (number of columns) and observations
col_num <- as.numeric(ncol(bangladesh_vars))
col_num_l1 <- col_num-1   # number of columns less 1 for the loop
row_num <- as.numeric(nrow(BPanel_reg))
# Loop time
for (n in 1:col_num_l1) {
data_var <- bangladesh_vars[,c(n, col_num)]              # create a data frame with just the variable to be differenced + wave
list_vars <- list()                             # create a list to store differenced variables
var_name <- colnames(bangladesh_vars[,n])           # get column name to store
names(bangladesh_vars)[names(bangladesh_vars) == var_name] <- "temp_name"
for (i in 1: row_num) {                               # for each observation
list  <- which(res[i,],  arr.ind = T)           # make a list of observations that are within a 100km radius
newdata <- data_var[list,]                        # create a new data frame with just these observations
year_same <- data_var[i,]$year              # identify the survey wave associated with the central observation
newdata <- subset(newdata, newdata$year==year_same) # leave only a subset of observations that are within the same year
newdata[, 1] <- sapply(newdata[, 1], as.numeric) # convert column to numeric
names(newdata)[names(newdata) == var_name] <- "test"  # change column name before getting the mean
avgvar100km <- mean(newdata$test)           # find the average of the variable in the 100 km radius
list_vars[[i]] <- avgvar100km - bangladesh_vars[i,]$temp_name
}
BPanel_reg$var <- as.numeric(list_vars)
names(BPanel_reg)[names(BPanel_reg) == 'var'] <- paste(var_name, "100km", sep="_")
}
View(data_var)
View(BPanel_reg)
# Prepare a data frame with just indicators of interest
bangladesh_vars <- BPanel_reg[, c("poor", "fatalitiesany1119", "drought_any111915", "floods_any1119", "hhsize", "hhage", "hhage2", "hhfemale", "relig", "hhedupri", "hhemp_agri", "nfe", "logAV", "lsT", "remitany", "region", "dum_flood", "dum_drought", "dum_violence", "trend_region", "year")]
# Get number of variables (number of columns) and observations
col_num <- as.numeric(ncol(bangladesh_vars))
col_num_l1 <- col_num-1   # number of columns less 1 for the loop
row_num <- as.numeric(nrow(BPanel_reg))
BPanel_reg <- as.data.frame(BPanel_reg)
# Loop time
for (n in 1:col_num_l1) {
data_var <- bangladesh_vars[,c(n, col_num)]              # create a data frame with just the variable to be differenced + wave
list_vars <- list()                             # create a list to store differenced variables
var_name <- colnames(bangladesh_vars[,n])           # get column name to store
names(bangladesh_vars)[names(bangladesh_vars) == var_name] <- "temp_name"
for (i in 1: row_num) {                               # for each observation
list  <- which(res[i,],  arr.ind = T)           # make a list of observations that are within a 100km radius
newdata <- data_var[list,]                        # create a new data frame with just these observations
year_same <- data_var[i,]$year              # identify the survey wave associated with the central observation
newdata <- subset(newdata, newdata$year==year_same) # leave only a subset of observations that are within the same year
newdata[, 1] <- sapply(newdata[, 1], as.numeric) # convert column to numeric
names(newdata)[names(newdata) == var_name] <- "test"  # change column name before getting the mean
avgvar100km <- mean(newdata$test)           # find the average of the variable in the 100 km radius
list_vars[[i]] <- avgvar100km - bangladesh_vars[i,]$temp_name
}
BPanel_reg$var <- as.numeric(list_vars)
names(BPanel_reg)[names(BPanel_reg) == 'var'] <- paste(var_name, "100km", sep="_")
}
View(bangladesh_vars)
bangladesh_vars <- BPanel_reg[, c("poor", "fatalitiesany1119", "drought_any111915", "floods_any1119", "hhsize", "hhage", "hhage2", "hhfemale", "relig", "hhedupri", "hhemp_agri", "nfe", "logAV", "lsT", "remitany", "region", "dum_flood", "dum_drought", "dum_violence", "trend_region", "year")]
View(BPanel_reg)
View(bangladesh_vars)
col_num <- as.numeric(ncol(bangladesh_vars))
col_num_l1 <- col_num-1   # number of columns less 1 for the loop
row_num <- as.numeric(nrow(BPanel_reg))
# Loop time
for (n in 1:col_num_l1) {
data_var <- bangladesh_vars[,c(n, col_num)]              # create a data frame with just the variable to be differenced + wave
list_vars <- list()                             # create a list to store differenced variables
var_name <- colnames(bangladesh_vars[,n])           # get column name to store
names(bangladesh_vars)[names(bangladesh_vars) == var_name] <- "temp_name"
for (i in 1: row_num) {                               # for each observation
list  <- which(res[i,],  arr.ind = T)           # make a list of observations that are within a 100km radius
newdata <- data_var[list,]                        # create a new data frame with just these observations
year_same <- data_var[i,]$year              # identify the survey wave associated with the central observation
newdata <- subset(newdata, newdata$year==year_same) # leave only a subset of observations that are within the same year
newdata[, 1] <- sapply(newdata[, 1], as.numeric) # convert column to numeric
names(newdata)[names(newdata) == var_name] <- "test"  # change column name before getting the mean
avgvar100km <- mean(newdata$test)           # find the average of the variable in the 100 km radius
list_vars[[i]] <- avgvar100km - bangladesh_vars[i,]$temp_name
}
BPanel_reg$var <- as.numeric(list_vars)
names(BPanel_reg)[names(BPanel_reg) == 'var'] <- paste(var_name, "100km", sep="_")
}
View(BPanel_reg)
View(bangladesh_vars)
View(data_var)
