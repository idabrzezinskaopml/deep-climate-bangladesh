---
title: "Spatially differencing key outcome variables Bangladesh"
output: html_document
date: "2024-11-14"
---
## Load relevant packages ##
---
```{r Load R libraries, include=FALSE}
library("plyr")                     # Load plyr package
library("dplyr")                   # Load dplyr package
library("readr")                    # Load readr package
library("tidyr")                    # Data manipulation
library("stargazer")                # For LaTeX tables
library("AER")                      # Robust standard errors
library("reshape2")                 # reshape long
library("haven")                    # Load dta files
library("foreign")                  # Export dta files
library("vctrs")                    # Vector operations
library("geosphere")                # To find distance between geographic coordinates
library("plm")                      # For panel data and first difference
library("sf")                       #Spatial vector data
library("stringr")                  # String operators

```


# Import .dta - strongly balanced BIHS panel for years 2011, 2015, 2019 with climatic and conflict variables merged in #

Notes from Vidya on which variables to use for differencing:

Variables used in the main multinomial logistic regression: qui mlogit pov4 i.fatalitiesany1119 i.drought_any111915 i.floods_any1119 hhsize hhage hhage2 hhfemale i.relig hhedupri hhemp_agri nfe logAV lsT remitany i.region [pw=hhweight], vce(cluster cluster) base(3)


```{r import BIHS panel}
# Set working directory for this chunk
setwd("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/BIHS data")

# Import .dta file with BIHS + drought and flood variables - to match with what Vidya has been using for regression.

BPanel_reg <- read_dta('BPanel_short.dta')

```

# Find centroids of unions #

```{r}
# Import shapefile for administrative level 4 harmonised with BIHS 
bgd_admin4 <- read_sf("C:/Users/idabr/OneDrive - Oxford Policy Management Limited/DEEP Conflict Climate Bangladesh/Data/Harmonised shapefile admin 4/bgd_admin4_bihs_harmonised.shp")

# Rename "ADM4_PCODE" as "uncode"
names(bgd_admin4)[names(bgd_admin4) == "ADM4_PCODE"] <- "uncode"

# Find the cenroid of the union
centroid_df <- st_centroid(bgd_admin4)

# Leave only identifier for admin 4
centroid_df <- centroid_df[, c("uncode", "ADM4_EN", "geometry")]

```

Create buffers around the centroids of unions - the differencing will be done within a 50km and 100km radius

```{r}
# Set geographical projection that uses meters
print(st_crs(bgd_admin4))
bgd_admin4 <- st_set_crs(bgd_admin4, 32646)

# Create buffer zones of 50km and 100km around the centroid and calculate the mean 
points_buffer_50km <- st_buffer(centroid_df, dist = 50000)
points_buffer_100km <- st_buffer(centroid_df, dist = 100000)

```

# Create indicators of interest for flood and drought

We need indicators that are assigned to specific survey years - such as whether there was a drought in the survey year

Note: using SPEI < -1.5 as the only definition of drought as it is consistent with the meteorological definition. With floods, there is only one definition, which is whether the household was residing in a particular union that was flooded

Given the BIHS data covers periods 2011, 2015, and 2019, but flood data from the GFD only goes until 2018 - I will assign floods from 2018 to the 2019 year of BIHS data.

```{r create drought/ flood indicators}
# Set dummy equal to 1 if household experiences a drought in the survey year 
BPanel_reg <- BPanel_reg %>% 
  mutate(dum_drought = case_when(
    (SPEI_2011 < -1.5 & year ==2011) | (SPEI_2015 < -1.5 & year ==2015) | (SPEI_2019 < -1.5 & year ==2019) ~ 1,
    .default = 0
  ))

# Have a look
summary(BPanel_reg$dum_drought)

# Replace for each survey year: wave 1 (2010), wave 2 (2012), wave 3 (2015), wave 4 (2019) - flood (union-level)

# Get column names for flood dates
colnames(BPanel_reg)

# First a variable capturing flood conditions if there was more than one in a year 
cols_to_sum_2011 <- grep("^f_2011", names(BPanel_reg))

# Sum the selected columns row-wise
BPanel_reg$flood_2011 <- rowSums(BPanel_reg[, cols_to_sum_2011])

# 2015
cols_to_sum_2015 <- grep("^f_2015", names(BPanel_reg))

# Sum the selected columns row-wise
BPanel_reg$flood_2015 <- rowSums(BPanel_reg[, cols_to_sum_2015])

# 2018
cols_to_sum_2018 <- grep("^f_2018", names(BPanel_reg))

# Sum the selected columns row-wise
BPanel_reg$flood_2018 <- rowSums(BPanel_reg[, cols_to_sum_2018])

# Create a dummy based on flood presence in a particular year 
BPanel_reg <- BPanel_reg %>% 
  mutate(dum_flood = case_when(
    (flood_2011 > 0 & year ==2011) | (flood_2015 > 0 & year ==2015) | (flood_2018 >0 & year ==2019) ~ 1,
    .default = 0
  ))

# Have a look
summary(BPanel_reg$dum_flood)

```

# Create variable for regional trends

```{r}
# Region
BPanel_reg$trend_region <- BPanel_reg$region*BPanel_reg$year

```

# Create dummies for violence

```{r conflict dummies}
# Create a dummy for whether there was a presence of conflict 
BPanel_reg$dum_violence <- 0

# Replace values if there were any fatalities
BPanel_reg$dum_violence[BPanel_reg$fatalitiesnew > 0] <- 1 
```

# Match BIHS data with 50km and 100km buffers

The differencing has to be done witin each year 

```{r}
# Change uncode to character
BPanel_reg$uncode <- as.character(BPanel_reg$uncode)

# Match compatible shapefile with BIHS data
BPanel_reg_centroids <- left_join(BPanel_reg, centroid_df)

# Turn into a spatial object
BPanel_reg_centroids <- st_as_sf(BPanel_reg_centroids)

# Function for extracting longitudes and latitudes
lonlat_fun <- function(x) {x %>% mutate(long=gsub(".*POINT (.+) ''.*", "\\1", geometry),
         cords=str_split_fixed(long,', ',2),
         longitude=as.numeric(gsub('c\\(','',cords[,1])),
         latitude=as.numeric(gsub('\\)','',cords[,2])))}

# Apply functions to all data frames
BPanel_reg <- lonlat_fun(BPanel_reg_centroids)

```


```{r}
# Create a data frame for just the coordinate points 
coordinates <- BPanel_reg[, c('longitude', 'latitude')]

# Remove geometry
coordinates <- as.data.frame(coordinates)
coordinates <- coordinates[,c("longitude", "latitude")]

# Put coordinates in a matrix 
coordsmatrix <- data.matrix(coordinates)

# Create a distance matrix for points within 100km 
res<- distm(coordsmatrix,fun=distGeo)<=100000

# Output is logical - either TRUE or FALSE depending on whether the condition is met 

# Create a distance matrix for points within 50km
res_50km<- distm(coordsmatrix,fun=distGeo)<=50000

# Check survey years
unique(BPanel_reg$year)
```


# SFD - 100 km radius version 

Loop to find all points within 100 km and calculate differenced outcomes

The code below will:
1. Loop through each row of the 'res' matrix and find columns with TRUE - that is observations that are within 100 km
2. Create a subset of observations within 100km 
3. Find the average of X and Y for those observations
4. Calculate the difference for each individual observation from the X,Y 100km average

Use the variable "poor" as outcome, which is the binary variable indicating

Loop is now automated (more or less) :)

```{r sfd 100 km}
# Turn into data frame
BPanel_reg <- as.data.frame(BPanel_reg)

# Prepare a data frame with just indicators of interest
bangladesh_vars <- BPanel_reg[, c("poor", "fatalitiesany1119", "drought_any111915", "floods_any1119", "hhsize", "hhage", "hhage2", "hhfemale", "relig", "hhedupri", "hhemp_agri", "nfe", "logAV", "lsT", "remitany", "region", "dum_flood", "dum_drought", "dum_violence", "trend_region", "year")]

# Get number of variables (number of columns) and observations
col_num <- as.numeric(ncol(bangladesh_vars))
col_num_l1 <- col_num-1   # number of columns less 1 for the loop
row_num <- as.numeric(nrow(BPanel_reg))


# Loop time 

for (n in 1:col_num_l1) {
  data_var <- bangladesh_vars[,c(n, col_num)]              # create a data frame with just the variable to be differenced + wave
  list_vars <- list()                             # create a list to store differenced variables 
  var_name <- colnames(bangladesh_vars[,n])           # get column name to store 
  names(bangladesh_vars)[names(bangladesh_vars) == var_name] <- "temp_name"

for (i in 1: row_num) {                               # for each observation
  list  <- which(res[i,],  arr.ind = T)           # make a list of observations that are within a 100km radius
  newdata <- data_var[list,]                        # create a new data frame with just these observations
  year_same <- data_var[i,]$year              # identify the survey wave associated with the central observation
  newdata <- subset(newdata, newdata$year==year_same) # leave only a subset of observations that are within the same year
  newdata[, 1] <- sapply(newdata[, 1], as.numeric) # convert column to numeric
  names(newdata)[names(newdata) == var_name] <- "test"  # change column name before getting the mean
  avgvar100km <- mean(newdata$test)           # find the average of the variable in the 100 km radius
  list_vars[[i]] <- avgvar100km - bangladesh_vars[i,]$temp_name       
}
  BPanel_reg$var <- as.numeric(list_vars)
  names(BPanel_reg)[names(BPanel_reg) == 'var'] <- paste(var_name, "100km", sep="_") 
}


```


SFD - 50km version

Note: given the size of the drought grid cell and definition of floods at union level, 50km spatial differencing might have insufficient variation - but let's try it

```{r sfd 50 km}
# Prepare a data frame with just indicators of interest
bangladesh_vars <- BPanel_reg[, c("poor", "fatalitiesany1119", "drought_any111915", "floods_any1119", "hhsize", "hhage", "hhage2", "hhfemale", "relig", "hhedupri", "hhemp_agri", "nfe", "logAV", "lsT", "remitany", "region", "dum_flood", "dum_drought", "dum_violence", "trend_region", "year")]

# Get number of variables (number of columns) and observations
col_num <- as.numeric(ncol(bangladesh_vars))
col_num_l1 <- col_num-1   # number of columns less 1 for the loop
row_num <- as.numeric(nrow(BPanel_reg))

# Loop time 

for (n in 1:col_num_l1) {
  data_var <- bangladesh_vars[,c(n, col_num)]              # create a data frame with just the variable to be differenced + wave
  list_vars <- list()                             # create a list to store differenced variables 
  var_name <- colnames(bangladesh_vars[,n])           # get column name to store 
  names(bangladesh_vars)[names(bangladesh_vars) == var_name] <- "temp_name"

for (i in 1: row_num) {                               # for each observation
  list  <- which(res_50km[i,],  arr.ind = T)           # make a list of observations that are within a 100km radius
  newdata <- data_var[list,]                        # create a new data frame with just these observations
  year_same <- data_var[i,]$year              # identify the survey wave associated with the central observation
  newdata <- subset(newdata, newdata$year==year_same) # leave only a subset of observations that are within the same year
  newdata[, 1] <- sapply(newdata[, 1], as.numeric) # convert column to numeric
  names(newdata)[names(newdata) == var_name] <- "test"  # change column name before getting the mean
  avgvar50km <- mean(newdata$test)           # find the average of the variable in the 100 km radius
  list_vars[[i]] <- avgvar50km - bangladesh_vars[i,]$temp_name       
}
  BPanel_reg$var <- as.numeric(list_vars)
  names(BPanel_reg)[names(BPanel_reg) == 'var'] <- paste(var_name, "50km", sep="_") 
}


```

# FE Regression test - SFD 100 km ##

## VIDYA'S PREFERRED VERSION ##

MAIN SPECIFICATION with a sub-set of controls 100km differencing 

FYI here are the variables I used in the main multinomial logistic regression: qui mlogit pov4 i.fatalitiesany1119 i.drought_any111915 i.floods_any1119 hhsize hhage hhage2 hhfemale i.relig hhedupri hhemp_agri nfe logAV lsT remitany i.region [pw=hhweight], vce(cluster cluster) base(3)

```{r}
# Drought
model20 <- plm(poor_100km ~ dum_drought_100km + hhsize_100km + hhage_100km + hhage2_100km + hhfemale_100km  +  hhedupri_100km + hhemp_agri_100km + nfe_100km + logAV_100km + lsT_100km + remitany_100km + trend_region_50km,
              data = BPanel_reg,
              index = c("HHID", "year"),       # select panel dimensions
             model = "within")

coeftest(model20, vcov=function(x) vcovHC(x, cluster="group", type="HC1"))

# Flood
model21 <- plm(poor_100km ~ dum_flood_100km + hhsize_100km + hhage_100km + hhage2_100km + hhfemale_100km  +  hhedupri_100km + hhemp_agri_100km + nfe_100km + logAV_100km + lsT_100km + remitany_100km + trend_region_50km,
              data = BPanel_reg,
              index = c("HHID", "year"),       # select panel dimensions
             model = "within")

coeftest(model21, vcov=function(x) vcovHC(x, cluster="group", type="HC1"))

# Violence
model22 <- plm(poor_100km ~ dum_violence_100km + hhsize_100km + hhage_100km + hhage2_100km + hhfemale_100km  +  hhedupri_100km + hhemp_agri_100km + nfe_100km + logAV_100km + lsT_100km + remitany_100km + trend_region_50km,
              data = BPanel_reg,
              index = c("HHID", "year"),       # select panel dimensions
             model = "within")

coeftest(model22, vcov=function(x) vcovHC(x, cluster="group", type="HC1"))


# Gather standard errors 
rob_se <- list(sqrt(diag(vcovHC(model20, type = "HC0", cluster = "group"))), sqrt(diag(vcovHC(model21, type = "HC0", cluster = "group"))), sqrt(diag(vcovHC(model22, type = "HC0", cluster = "group"))))

# Publication-style table 
stargazer(model20, model21, model22,
          se = rob_se,
          keep.stat = c("n", "rsq"),
          type = "html",
          title = "Poverty and crises in Bangladesh",
          notes = "Household and year fixed effects applied and robust standard errors used in all specifications.",
          out = "bangladesh_reg.html")

```

# FE Regression test - SFD 50 km ##

```{r sfd 50 km}
# Drought
model23 <- plm(poor_50km ~ dum_drought_50km + hhsize_50km + hhage_50km + hhage2_50km + hhfemale_50km  +  hhedupri_50km + hhemp_agri_50km + nfe_50km + logAV_50km + lsT_50km + remitany_50km + trend_region_50km,
              data = BPanel_reg,
              index = c("HHID", "year"),       # select panel dimensions
             model = "within")

coeftest(model23, vcov=function(x) vcovHC(x, cluster="group", type="HC1"))

# Flood
model24 <- plm(poor_50km ~ dum_flood_50km + hhsize_50km + hhage_50km + hhage2_50km + hhfemale_50km  +  hhedupri_50km + hhemp_agri_50km + nfe_50km + logAV_50km + lsT_50km + remitany_50km + trend_region_50km,
              data = BPanel_reg,
              index = c("HHID", "year"),       # select panel dimensions
             model = "within")

coeftest(model24, vcov=function(x) vcovHC(x, cluster="group", type="HC1"))

# Violence
model25 <- plm(poor_50km ~ dum_violence_50km + hhsize_50km + hhage_50km + hhage2_50km + hhfemale_50km  +  hhedupri_50km + hhemp_agri_50km + nfe_50km + logAV_50km + lsT_50km + remitany_50km + trend_region_50km,
              data = BPanel_reg,
              index = c("HHID", "year"),       # select panel dimensions
             model = "within")

coeftest(model25, vcov=function(x) vcovHC(x, cluster="group", type="HC1"))


# Gather standard errors 
rob_se1 <- list(sqrt(diag(vcovHC(model23, type = "HC0", cluster = "group"))), sqrt(diag(vcovHC(model24, type = "HC0", cluster = "group"))), sqrt(diag(vcovHC(model25, type = "HC0", cluster = "group"))))

# Publication-style table 
stargazer(model23, model24, model25,
          se = rob_se1,
          keep.stat = c("n", "rsq"),
          type = "html",
          title = "Poverty and crises in Bangladesh",
          notes = "Household and year fixed effects applied and robust standard errors used in all specifications.",
          out = "bangladesh_reg50km.html")
```

